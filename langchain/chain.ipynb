{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fd20ce-5722-42bb-aca6-f6944f9a6572",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.output_parsers import StrOutputParser, PydanticOutputParser, JsonOutputParser, XMLOutputParser, BaseOutputParser\n",
    "from langchain_core.runnables import RunnableParallel, RunnableLambda, RunnablePassthrough, ConfigurableField, chain\n",
    "from langchain_core.exceptions import OutputParserException\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.runnables.hub import HubRunnable\n",
    "from pydantic import BaseModel, Field, model_validator\n",
    "from typing import List, Iterator\n",
    "from pprint import pprint\n",
    "import json\n",
    "from operator import itemgetter\n",
    "\n",
    "# base_url = \"https://api.deepseek.com/\"\n",
    "\n",
    "# model=\"deepseek-chat\"\n",
    "# llm = ChatOpenAI(model=model, temperature=0, api_key=api_key, base_url=base_url)\n",
    "\n",
    "base_url=\"https://ark.cn-beijing.volces.com/api/v3\"\n",
    "\n",
    "model=\"doubao-1.5-pro-32k-250115\"\n",
    "llm = ChatOpenAI(model=model, temperature=0, api_key=api_key, base_url=base_url).configurable_fields(\n",
    "    temperature=ConfigurableField(\n",
    "        id=\"llm_temperature\",\n",
    "        name=\"LLM Temperature\",\n",
    "        description=\"The temprature of the LLM\",\n",
    "    )\n",
    ")\n",
    "\n",
    "# base_url=\"http://localhost:11434\"\n",
    "# model=\"deepseek-r1\"\n",
    "# llm = ChatOllama(model=model, temperature=0, base_url=base_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "490cb90e-b206-48f2-bf14-ddb906725a6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here| is| a| funny| joke| about| a| par|rot|:|\n",
      "\n",
      "|A| man| walks| into| a| pet| store| and| asks| to| see| the| talking| parro|ts|.| The| store| owner| shows| him| two| beautiful| birds|.|\n",
      "\n",
      "|\"|Here|,| this| one| is| $|5|,|0|0|0| and| the| other| is| $|1|0|,|0|0|0|,\"| says| the| owner|.|\n",
      "\n",
      "|The| customer| is| shocked| and| asks|,| \"|Why| is| the| more| expensive| one| twice| the| price| of| the| other|?| Do| they| talk| differently|?\"|\n",
      "\n",
      "|The| owner| replies|,| \"|No|,| they| both| talk| equally| well|.| The| $|1|0|,|0|0|0| par|rot| has| better| management| skills|.| The| $|5|,|0|0|0| par|rot| just| says|,| '|I|'m| a| par|rot|,'| but| the| $|1|0|,|0|0|0| par|rot| stands| up|,| looks| around|,| and| says|,| '|Well|,| it| looks| like| we|'re| all| out| of| parro|ts| today|!|'\"| ||"
     ]
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")\n",
    "parser = StrOutputParser()\n",
    "chain = prompt | llm | parser\n",
    "async for chunk in chain.astream({\"topic\": \"parrot\"}):\n",
    "    print(chunk, end=\"|\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99671549-5c76-449e-9994-7baa4a1f23d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "{'countries': []}\n",
      "{'countries': [{}]}\n",
      "{'countries': [{'name': ''}]}\n",
      "{'countries': [{'name': '法国'}]}\n",
      "{'countries': [{'name': '法国', 'population': 6}]}\n",
      "{'countries': [{'name': '法国', 'population': 67}]}\n",
      "{'countries': [{'name': '法国', 'population': 678}]}\n",
      "{'countries': [{'name': '法国', 'population': 6781}]}\n",
      "{'countries': [{'name': '法国', 'population': 67813}]}\n",
      "{'countries': [{'name': '法国', 'population': 678133}]}\n",
      "{'countries': [{'name': '法国', 'population': 6781339}]}\n",
      "{'countries': [{'name': '法国', 'population': 67813396}]}\n",
      "{'countries': [{'name': '法国', 'population': 67813396}, {}]}\n",
      "{'countries': [{'name': '法国', 'population': 67813396}, {'name': ''}]}\n",
      "{'countries': [{'name': '法国', 'population': 67813396}, {'name': '西班牙'}]}\n",
      "{'countries': [{'name': '法国', 'population': 67813396}, {'name': '西班牙', 'population': 4}]}\n",
      "{'countries': [{'name': '法国', 'population': 67813396}, {'name': '西班牙', 'population': 47}]}\n",
      "{'countries': [{'name': '法国', 'population': 67813396}, {'name': '西班牙', 'population': 474}]}\n",
      "{'countries': [{'name': '法国', 'population': 67813396}, {'name': '西班牙', 'population': 4745}]}\n",
      "{'countries': [{'name': '法国', 'population': 67813396}, {'name': '西班牙', 'population': 47450}]}\n",
      "{'countries': [{'name': '法国', 'population': 67813396}, {'name': '西班牙', 'population': 474507}]}\n",
      "{'countries': [{'name': '法国', 'population': 67813396}, {'name': '西班牙', 'population': 4745079}]}\n",
      "{'countries': [{'name': '法国', 'population': 67813396}, {'name': '西班牙', 'population': 47450795}]}\n",
      "{'countries': [{'name': '法国', 'population': 67813396}, {'name': '西班牙', 'population': 47450795}, {}]}\n",
      "{'countries': [{'name': '法国', 'population': 67813396}, {'name': '西班牙', 'population': 47450795}, {'name': ''}]}\n",
      "{'countries': [{'name': '法国', 'population': 67813396}, {'name': '西班牙', 'population': 47450795}, {'name': '日本'}]}\n",
      "{'countries': [{'name': '法国', 'population': 67813396}, {'name': '西班牙', 'population': 47450795}, {'name': '日本', 'population': 1}]}\n",
      "{'countries': [{'name': '法国', 'population': 67813396}, {'name': '西班牙', 'population': 47450795}, {'name': '日本', 'population': 12}]}\n",
      "{'countries': [{'name': '法国', 'population': 67813396}, {'name': '西班牙', 'population': 47450795}, {'name': '日本', 'population': 125}]}\n",
      "{'countries': [{'name': '法国', 'population': 67813396}, {'name': '西班牙', 'population': 47450795}, {'name': '日本', 'population': 1259}]}\n",
      "{'countries': [{'name': '法国', 'population': 67813396}, {'name': '西班牙', 'population': 47450795}, {'name': '日本', 'population': 12596}]}\n",
      "{'countries': [{'name': '法国', 'population': 67813396}, {'name': '西班牙', 'population': 47450795}, {'name': '日本', 'population': 125960}]}\n",
      "{'countries': [{'name': '法国', 'population': 67813396}, {'name': '西班牙', 'population': 47450795}, {'name': '日本', 'population': 1259600}]}\n",
      "{'countries': [{'name': '法国', 'population': 67813396}, {'name': '西班牙', 'population': 47450795}, {'name': '日本', 'population': 12596000}]}\n",
      "{'countries': [{'name': '法国', 'population': 67813396}, {'name': '西班牙', 'population': 47450795}, {'name': '日本', 'population': 125960000}]}\n"
     ]
    }
   ],
   "source": [
    "chain = (\n",
    "    llm | JsonOutputParser()\n",
    ")\n",
    "async for text in chain.astream(\n",
    "    \"以JSON格式输出法国，西班牙和日本的国家列表和人口数量。\"\n",
    "    '使用一个字典，包含外层键名为\"countries\"的国家列。'\n",
    "    '每个国家应该有键名\"name\"和\"population\"'\n",
    "):\n",
    "    print(text, flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "56db5a89-7ce0-4d0e-afdc-ab957e07197a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on_chain_start\n",
      "on_chat_model_start\n",
      "on_chat_model_stream\n",
      "on_parser_start\n",
      "on_chat_model_stream\n",
      "on_chat_model_stream\n",
      "on_chat_model_stream\n",
      "on_chat_model_stream\n",
      "on_chat_model_stream\n",
      "on_chat_model_stream\n",
      "on_chat_model_stream\n",
      "on_chat_model_stream\n",
      "on_chat_model_stream\n",
      "on_chat_model_end\n",
      "on_parser_end\n",
      "on_chain_end\n",
      "Total 16 events\n"
     ]
    }
   ],
   "source": [
    "events = []\n",
    "async for event in chain.astream_events(\"hello\"):\n",
    "    print(event[\"event\"], flush=True)\n",
    "    events.append(event)\n",
    "print(f\"Total {len(events)} events\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f1fe5115-1e4b-474e-987d-5c2d20ea3f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               +--------------------------+                \n",
      "               | Parallel<joke,poem>Input |                \n",
      "               +--------------------------+                \n",
      "                   ***               ***                   \n",
      "                ***                     ***                \n",
      "              **                           **              \n",
      "+--------------------+              +--------------------+ \n",
      "| ChatPromptTemplate |              | ChatPromptTemplate | \n",
      "+--------------------+              +--------------------+ \n",
      "           *                                   *           \n",
      "           *                                   *           \n",
      "           *                                   *           \n",
      "    +------------+                      +------------+     \n",
      "    | ChatOpenAI |                      | ChatOpenAI |     \n",
      "    +------------+*                     +------------+     \n",
      "                   ***               ***                   \n",
      "                      ***         ***                      \n",
      "                         **     **                         \n",
      "              +---------------------------+                \n",
      "              | Parallel<joke,poem>Output |                \n",
      "              +---------------------------+                \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[ChatPromptTemplate(input_variables=['topic'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['topic'], input_types={}, partial_variables={}, template='给我讲一个关于{topic}的笑话'), additional_kwargs={})]),\n",
       " ChatPromptTemplate(input_variables=['topic'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['topic'], input_types={}, partial_variables={}, template='给我写一首关于{topic}的绝句'), additional_kwargs={})])]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joke_chain = ChatPromptTemplate.from_template(\"给我讲一个关于{topic}的笑话\") | llm\n",
    "poem_chain = (\n",
    "    ChatPromptTemplate.from_template(\"给我写一首关于{topic}的绝句\") | llm\n",
    ")\n",
    "main_chain = RunnableParallel(joke=joke_chain, poem=poem_chain)\n",
    "main_chain.get_graph().print_ascii()\n",
    "main_chain.get_prompts()\n",
    "main_chain.invoke({\"topic\": \"程序员\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7d515d20-0bbf-452e-9fd5-11ec6a2925cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'这两个笑话的主题都围绕着熊和毛发展开，主要体现出一种荒诞、意外的幽默风格。\\n\\n### “北极熊拔毛”笑话主题\\n - 该笑话主题侧重于展现一种无厘头的行为和意外的结果。北极熊出于无聊进行拔毛这一荒诞行为，当它把毛拔光后才感受到寒冷，这种行为与结果之间的逻辑反差产生了幽默效果，主题核心是凸显这种无意义行为带来的意外后果，以荒诞情景引人发笑。\\n\\n### “熊和兔子”笑话主题\\n - 此笑话主题同样是利用意外和荒诞元素制造笑点。熊询问兔子是否掉毛，看似是一个正常的交流，但它询问的目的却是抓起兔子擦屁股，这一意外的发展打破了常规的思维逻辑，兔子的回答和熊的实际行为之间形成了强烈反差，主题在于通过这种意外情节来营造幽默氛围。 '"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt1 = ChatPromptTemplate.from_template(\"给我讲一个关于{topic}的笑话\") \n",
    "prompt2 = ChatPromptTemplate.from_template(\"这个笑话的主题是什么：{joke}\") \n",
    "\n",
    "@chain\n",
    "def custom_chain(text):\n",
    "    prompt_val1 = prompt1.invoke({\"topic\": text})\n",
    "    output1 = llm.invoke(prompt_val1)\n",
    "    parsed_output1 = StrOutputParser().invoke(output1)\n",
    "\n",
    "    chain2 = prompt2 | llm | StrOutputParser()\n",
    "\n",
    "    return chain2.invoke({\"joke\": parsed_output1})\n",
    "\n",
    "custom_chain.invoke(\"熊\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fc2d8e57-800d-4c07-82d6-04b8af3b7e16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='To find the sum of 3 and 12, you simply add the two numbers together.\\n\\n3 + 12 = 15\\n\\nSo, the result is 15. ', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 41, 'prompt_tokens': 16, 'total_tokens': 57, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': None, 'reasoning_tokens': 0, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'doubao-1-5-pro-32k-250115', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--117fc9a2-14f6-4d89-adc7-3273291f6387-0', usage_metadata={'input_tokens': 16, 'output_tokens': 41, 'total_tokens': 57, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def length_function(text):\n",
    "    return len(text)\n",
    "\n",
    "def _multiple_length_function(text1, text2):\n",
    "    return len(text1)*len(text2)\n",
    "\n",
    "def multiple_length_function(_dict):\n",
    "    return _multiple_length_function(_dict[\"text1\"], _dict[\"text2\"])\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"what is {a} + {b}\")\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"a\": itemgetter(\"foo\") | RunnableLambda(length_function),\n",
    "        \"b\": {\"text1\": itemgetter(\"foo\"), \"text2\": itemgetter(\"bar\")} | RunnableLambda(multiple_length_function)\n",
    "    }\n",
    "    | prompt | llm\n",
    ")\n",
    "chain.invoke({\"foo\": \"bar\", \"bar\": \"gah3\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8e0ddcec-1dfb-4756-8245-a18bfcf502db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "熊猫，棕熊，北极熊，浣熊，貂熊\n",
      "['熊猫']\n",
      "['北极熊']\n",
      "['棕熊']\n",
      "['黑熊']\n",
      "['马来熊']\n"
     ]
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\"请列出5个与以下动物相似的动物名称，用逗号分隔：{animal}。不要包含数字\")\n",
    "str_chain = prompt | llm | StrOutputParser()\n",
    "for chunk in str_chain.stream({\"animal\": \"熊\"}):\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "print()\n",
    "\n",
    "def split_into_list(input: Iterator[str]) -> Iterator[List[str]]:\n",
    "    buffer = \"\"\n",
    "    for chunk in input:\n",
    "        buffer += chunk;\n",
    "        while \"，\" in buffer:\n",
    "            comma_index = buffer.index(\"，\")\n",
    "            yield [buffer[:comma_index].strip()]\n",
    "            buffer = buffer[comma_index+1:]\n",
    "    yield [buffer.strip()]\n",
    "\n",
    "list_chain = str_chain | split_into_list\n",
    "for chunk in list_chain.stream({\"animal\": \"熊\"}):\n",
    "    print(chunk, end=\"\\n\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c40fcf4c-3182-4b17-a569-b26f221b0ec3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'passed': {'num': 1}, 'modified': 2}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runnable = RunnableParallel(\n",
    "    passed=RunnablePassthrough(),\n",
    "    modified=lambda x:x[\"num\"]+1\n",
    ")\n",
    "runnable.invoke({\"num\":1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5510f38-5904-4d3e-9c83-c1aaff2ff1ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='以下为你列举不同类型的哺乳动物名称：\\n### 陆地哺乳动物\\n- **老虎**：老虎是世界上最大的猫科动物之一，也是顶级的食肉动物。它拥有强壮的身体、锋利的爪子和牙齿，捕猎能力极强。老虎主要分布在亚洲的部分地区，由于栖息地破坏和偷猎等原因，许多老虎种类都面临濒危的困境。\\n- **大象**：大象是陆地上最大的哺乳动物，拥有庞大而强壮的身体，长长的鼻子（象鼻）非常灵活，可用于抓取物体、喝水等。大象是草食性动物，主要分布在非洲和亚洲的一些地区。\\n- **大熊猫**：大熊猫是中国特有的珍稀动物，憨态可掬，深受人们喜爱。它们主要以竹子为食，有着圆滚滚的身体和黑白相间的毛色，其栖息地主要在中国四川、陕西和甘肃等地的山区。\\n### 海洋哺乳动物\\n- **海豚**：海豚是一种非常聪明且可爱的海洋哺乳动物。它们具有流线型的身体，游泳速度快，生活在世界各大洋的温暖海域。海豚喜欢群居，通过发出各种声音进行交流，还能表演各种高难度的动作。\\n- **鲸鱼**：鲸鱼是海洋中的巨型生物，其中蓝鲸是地球上现存体型最大的动物。鲸鱼用肺呼吸，需要定期浮出水面换气。不同种类的鲸鱼食性有所不同，有的主要以小鱼、小虾为食，有的则捕食大型海洋生物。\\n### 空中哺乳动物\\n- **蝙蝠**：蝙蝠是唯一能够真正飞行的哺乳动物。它们的前肢进化成了翅膀，通常在夜间活动，依靠回声定位来捕食昆虫、水果等食物。蝙蝠种类繁多，广泛分布于世界各地。 ', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 366, 'prompt_tokens': 15, 'total_tokens': 381, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': None, 'reasoning_tokens': 0, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'doubao-1-5-pro-32k-250115', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--8ba50de8-9475-477b-b652-67d24bac077a-0', usage_metadata={'input_tokens': 15, 'output_tokens': 366, 'total_tokens': 381, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# llm.invoke(\"请给我一个哺乳动物的名称\")\n",
    "llm.with_config(configurable={\"llm_temperature\": 0.9}).invoke(\"请给我一个哺乳动物的名称\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c8e3eead-ffe8-496c-8366-e30ae3e590c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bytedance/miniconda3/envs/langchain/lib/python3.12/site-packages/langsmith/client.py:272: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[HumanMessage(content=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: bar \\nContext: gah3 \\nAnswer:\", additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = HubRunnable(\"rlm/rag-prompt\").configurable_fields(\n",
    "    owner_repo_commit=ConfigurableField(\n",
    "        id=\"hub_commit\",\n",
    "        name=\"Hub Commit\",\n",
    "        description=\"The Hub commit to pull from\",\n",
    "    )\n",
    ")\n",
    "prompt.invoke({\"question\": \"bar\", \"context\": \"gah3\"})\n",
    "# prompt.with_config(configurable={\"hub_commit\": \"rlm/rag-prompt-llama\"}).invoke({\"question\": \"bar\", \"context\": \"gah3\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa677853-2fb8-4c28-9428-08c8ef6dcaba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
